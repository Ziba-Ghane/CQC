{"cells":[{"cell_type":"markdown","id":"bb16ef61","metadata":{"id":"bb16ef61"},"source":["# Set Variables"]},{"cell_type":"code","execution_count":null,"id":"1d473d55","metadata":{"ExecuteTime":{"end_time":"2022-05-16T18:25:29.443215Z","start_time":"2022-05-16T18:25:29.419228Z"},"id":"1d473d55"},"outputs":[],"source":["output_version = 1\n","\n","# -------- dataset\n","# software_name = \"Camel\"\n","# software_name = \"CloudStack\"\n","# software_name = \"Geode\"\n","software_name = \"Hbase\"\n","\n","token_threshold = 20000\n","\n","\n","# -------- my_keyword_Based & my_docMaxLen\n","my_keyword_Based = True\n","# my_keyword_Based = False\n","my_docMaxLen = 100 if my_keyword_Based else None"]},{"cell_type":"code","source":["dataset_file_names = {\n","    \"Camel\":      \"Camel_DE - v.02\",\n","    \"CloudStack\": \"CloudStack_DE - v.01\",\n","    \"Geode\":      \"Geode_DE - v.01\",\n","    \"Hbase\":      \"Hbase_DE - v.01\"\n","}\n","\n","dataset_file_name = dataset_file_names[software_name]"],"metadata":{"id":"60YwB3Q6EAlN"},"id":"60YwB3Q6EAlN","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"60798d75","metadata":{"id":"60798d75"},"source":["# Google Colab"]},{"cell_type":"code","source":["# Libs\n","!pip install enlighten\n","!pip install --upgrade matplotlib"],"metadata":{"id":"8-vmcwgTEDH_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1658330504042,"user_tz":-270,"elapsed":9749,"user":{"displayName":"Ziba Ghane","userId":"08687858434532704051"}},"outputId":"59e718c8-5010-4af6-df80-91d56bc16f8e"},"id":"8-vmcwgTEDH_","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: enlighten in /usr/local/lib/python3.7/dist-packages (1.10.2)\n","Requirement already satisfied: blessed>=1.17.7 in /usr/local/lib/python3.7/dist-packages (from enlighten) (1.19.1)\n","Requirement already satisfied: prefixed>=0.3.2 in /usr/local/lib/python3.7/dist-packages (from enlighten) (0.3.2)\n","Requirement already satisfied: wcwidth>=0.1.4 in /usr/local/lib/python3.7/dist-packages (from blessed>=1.17.7->enlighten) (0.2.5)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from blessed>=1.17.7->enlighten) (1.15.0)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (3.5.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (0.11.0)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (4.34.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (21.3)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (7.1.2)\n","Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (3.0.9)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.4.3)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.8.2)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.21.6)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib) (4.1.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7->matplotlib) (1.15.0)\n"]}]},{"cell_type":"code","source":["# load data from google drive\n","from google.colab import drive\n","drive.mount(\"/content/gdrive\", force_remount=True)\n","!ls \"/content/gdrive/My Drive/\""],"metadata":{"id":"zPHJVcLLEEqn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1658330509560,"user_tz":-270,"elapsed":5543,"user":{"displayName":"Ziba Ghane","userId":"08687858434532704051"}},"outputId":"1b2ac5f8-38a7-4378-f620-306b6a5dbdd1"},"id":"zPHJVcLLEEqn","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n","'Colab Notebooks'   SAVE\n"]}]},{"cell_type":"code","source":["# project folder path\n","project_folder = \"gdrive/MyDrive/Colab Notebooks/paper/\"\n","\n","# data folder path\n","data_subfolder_1 =    \"00- My Data/one-phase method/\"\n","\n","# output folder path\n","output_subfolder_1 =  \"01- Jupyter Notebook/10- Naive Bayes - MultinomialNB/00. Output/\""],"metadata":{"id":"_vVKXenUEGI7"},"id":"_vVKXenUEGI7","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# dataset folder path\n","subfolder_2 = software_name + \"/\"\n","subfolder_3 = dataset_file_name + \"/\"\n","\n","# output data-folder path\n","output_folder = project_folder + output_subfolder_1 + subfolder_2 + subfolder_3\n","\n","data_folder_dataset = project_folder + data_subfolder_1 + subfolder_2\n","data_folder_w2v     = project_folder + data_subfolder_1"],"metadata":{"id":"OUx3DfoSEHVc"},"id":"OUx3DfoSEHVc","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"a06904c1","metadata":{"id":"a06904c1"},"source":["# Libs"]},{"cell_type":"code","execution_count":null,"id":"b5844ea3","metadata":{"ExecuteTime":{"end_time":"2022-05-16T18:25:39.708014Z","start_time":"2022-05-16T18:25:29.540159Z"},"id":"b5844ea3"},"outputs":[],"source":["import string\n","import re\n","import json\n","import os.path\n","import os\n","\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import enlighten\n","\n","from collections import Counter, OrderedDict\n","from operator import truediv\n","\n","from torchvision import transforms\n","from nltk.corpus import stopwords\n","from nltk.tokenize import WordPunctTokenizer\n","from nltk.text import TextCollection\n","from matplotlib.ticker import MaxNLocator\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.metrics.pairwise import linear_kernel\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.naive_bayes import MultinomialNB"]},{"cell_type":"code","execution_count":null,"id":"9d0015ab","metadata":{"ExecuteTime":{"end_time":"2022-05-16T18:25:39.724005Z","start_time":"2022-05-16T18:25:39.711010Z"},"id":"9d0015ab","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1658330511719,"user_tz":-270,"elapsed":30,"user":{"displayName":"Ziba Ghane","userId":"08687858434532704051"}},"outputId":"385810cb-7618-4532-b561-836ba62da691"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":8}],"source":["import nltk\n","nltk.download(\"stopwords\")"]},{"cell_type":"markdown","id":"a2d03d3d","metadata":{"id":"a2d03d3d"},"source":["# Config"]},{"cell_type":"code","execution_count":null,"id":"a124d3bd","metadata":{"ExecuteTime":{"end_time":"2022-05-16T18:25:39.878410Z","start_time":"2022-05-16T18:25:39.851444Z"},"id":"a124d3bd"},"outputs":[],"source":["mypaths = {\n","    \"data\": {\n","        \"dataset\":            data_folder_dataset + dataset_file_name + \".csv\",\n","        \"tfidf_word_weights\": data_folder_dataset + dataset_file_name + \" _ tfidf-word-weights-v01.json\",\n","        \"w2v_word_vectors\":   data_folder_w2v     + \"w2vGoogle.bin\"\n","    },\n","    \"output\": {\n","        \"not_keyword_based\": {\n","            \"performance\": output_folder + \"MultiNB-w2v-NKB-performance-v{}.json\".format(output_version)\n","        },\n","        \"keyword_based\": {\n","            \"performance\": output_folder + \"MultiNB-w2v-KB-performance-v{}.json\".format(output_version)\n","        }\n","    }\n","}\n","\n","preprocessing_params = {\n","    \"data\":{\n","        \"dataset\": {\n","            \"columns_name\":   [\"text\", \"bug_class_2\"],\n","            \"columns_dtype\" : {0: \"str\", 1: \"int64\"},\n","            \"bug_classes\": [0, 1],\n","            \"num_bug_classes\": 2\n","        }\n","    },\n","    \"keyword_Based\": my_keyword_Based,\n","    \"docMaxLen\": my_docMaxLen\n","}"]},{"cell_type":"code","execution_count":null,"id":"e5592bc7","metadata":{"ExecuteTime":{"end_time":"2022-05-16T18:25:39.893401Z","start_time":"2022-05-16T18:25:39.882407Z"},"id":"e5592bc7"},"outputs":[],"source":["bcd_colours = [\"blue\", \"green\", \"red\"]"]},{"cell_type":"markdown","id":"7fb3d4d6","metadata":{"id":"7fb3d4d6"},"source":["# Read File"]},{"cell_type":"code","execution_count":null,"id":"608e5c63","metadata":{"ExecuteTime":{"end_time":"2022-05-16T18:25:41.177254Z","start_time":"2022-05-16T18:25:39.928233Z"},"id":"608e5c63"},"outputs":[],"source":["df_main = pd.read_csv(\n","    mypaths[\"data\"][\"dataset\"], \n","    names=preprocessing_params[\"data\"][\"dataset\"][\"columns_name\"], \n","    dtype=preprocessing_params[\"data\"][\"dataset\"][\"columns_dtype\"],\n","    header=None, \n","    skip_blank_lines=True\n",")"]},{"cell_type":"markdown","id":"c7071c0b","metadata":{"id":"c7071c0b"},"source":["# Compose"]},{"cell_type":"code","execution_count":null,"id":"c1ddd8d7","metadata":{"ExecuteTime":{"end_time":"2022-05-16T18:25:39.925235Z","start_time":"2022-05-16T18:25:39.897399Z"},"id":"c1ddd8d7"},"outputs":[],"source":["class Rows(object):\n","    def __init__(self, columns_name, bug_classes):\n","        self.columns_name = columns_name\n","        self.bug_classes = bug_classes\n","    \n","    \n","    def __call__(self, df):\n","        # 1. Set cells to None that have just white spaces\n","        df = df.apply(self.white_spaces_to_None_, axis=1)\n","        \n","        # 2. Delete rows that have NaN values in each of its columns\n","        df.dropna(axis=0, how=\"any\", subset=self.columns_name, inplace=True)\n","        \n","        # 3. Delete rows with class value other than [0, 1]\n","        indexNames = df[~df[\"bug_class_2\"].isin(self.bug_classes)].index\n","        df.drop(indexNames, axis=0, inplace=True)\n","        \n","        return df\n","    \n","    \n","    # set columns that just have white spaces to None\n","    def white_spaces_to_None_(self, row):\n","        for i in self.columns_name:\n","            if row[i] and len(str(row[i]).strip()) == 0:\n","                row[i] = None\n","        return row"]},{"cell_type":"markdown","source":["## obj"],"metadata":{"id":"3D_BqgR8RVxt"},"id":"3D_BqgR8RVxt"},{"cell_type":"code","execution_count":null,"id":"23eacebc","metadata":{"ExecuteTime":{"end_time":"2022-05-16T18:25:42.144387Z","start_time":"2022-05-16T18:25:41.184196Z"},"id":"23eacebc"},"outputs":[],"source":["composed_pre = transforms.Compose([\n","    Rows(\n","        preprocessing_params[\"data\"][\"dataset\"][\"columns_name\"], \n","        preprocessing_params[\"data\"][\"dataset\"][\"bug_classes\"]\n","    )\n","])\n","\n","df_main = composed_pre(df_main)"]},{"cell_type":"code","execution_count":null,"id":"854cfe7b","metadata":{"ExecuteTime":{"end_time":"2022-05-16T18:25:42.192360Z","start_time":"2022-05-16T18:25:42.150384Z"},"id":"854cfe7b"},"outputs":[],"source":["texts = df_main[\"text\"].tolist()\n","labels = df_main[\"bug_class_2\"].tolist()"]},{"cell_type":"markdown","source":["# IV. ProgressLines"],"metadata":{"id":"jjpKbL2jFNLU"},"id":"jjpKbL2jFNLU"},{"cell_type":"code","source":["class ProgressLines():\n","    \n","    def progress_lines(self, num, total, description, unit, colour):\n","        desc = self.set_strings_to_equal_len_(description)\n","        manager = enlighten.get_manager()\n","        progresses = []\n","        for i in range(num):\n","            prog = manager.counter(total=total[i], desc=desc[i], unit=unit[i], color=colour[i])\n","            prog.refresh()\n","            progresses.append(prog)\n","        self.progresses = progresses\n","    \n","    \n","    def set_strings_to_equal_len_(self, description):\n","        max_len = 0\n","        # longest_string_length = len(max(description, key=len))\n","        longest_string_length = -1\n","        for ele in description:\n","            if len(ele) > longest_string_length:\n","                longest_string_length = len(ele)\n","        w = []\n","        for i, word in enumerate(description):\n","            temp = longest_string_length - len(word)\n","            w.append(word + \" \" * temp)\n","        return w"],"metadata":{"id":"Ds6y30AOFMSV"},"id":"Ds6y30AOFMSV","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"39f184a2","metadata":{"id":"39f184a2"},"source":["# I. Preprocessing"]},{"cell_type":"code","execution_count":null,"id":"dff0d6c6","metadata":{"ExecuteTime":{"end_time":"2022-05-16T18:25:42.709879Z","start_time":"2022-05-16T18:25:42.652912Z"},"id":"dff0d6c6"},"outputs":[],"source":["class Preprocessing():\n","    \n","    my_deleted_bug = {}\n","    \n","    docMaxLen = 0 # max keywords allowed\n","    w2vDic = {} # dic : {\"w1\": [0.1, 0.2, ...], \"w2\": [0.1, 0.3, ...], ...}\n","    paddingVector = np.zeros(300, dtype=\"float32\")\n","    bugRepTokens = [] # [[w1, w2, w3, ...], [w1, w2, ...], ...]\n","    docMaxTokenNo_org = 0\n","    docMaxTokenNo_token_threshold = 0\n","    docMaxTokenNo = 0 # max doc len after vectorization\n","    vector_tfidf = [] # array of dictinaries: [{\"w1\": 0.1, \"w2\": 0.3, ...}, {}, ...]\n","    vector_em = [] # array of matrix : [ [w1Vector, w2Vector], [], ...] \n","    \n","    \n","    def __init__(self, docMaxLen, token_threshold):\n","        self.docMaxLen = docMaxLen\n","        self.token_threshold = token_threshold\n","    \n","    \n","    # tfidf of corpuses words\n","    def load_tfidf(self, tfidf_path):\n","        with open(tfidf_path, \"r\") as filehandle:\n","            self.vector_tfidf = json.load(filehandle)\n","    \n","    \n","    def tokenize(self, texts):\n","        stop_words = set(stopwords.words(\"english\"))\n","        excludedTokens = {\"http\", \"url\", \"https\"}\n","        \n","        # self.df.columns[0] : \"description\"\n","        for i, doc in enumerate(texts):\n","            thisTokens = []\n","            doc = doc.lower()\n","            for token in WordPunctTokenizer().tokenize(doc):\n","                if (token in string.punctuation or token in stop_words or token in excludedTokens or \n","                    (not re.findall(\"\\w\", token)) or re.findall(\"\\A[0-9]\", token)):\n","                    continue\n","                thisTokens.append(token)\n","                self.w2vDic[token] = self.paddingVector\n","            if len(thisTokens) <= self.token_threshold:\n","                self.bugRepTokens.append(thisTokens)\n","                if (len(thisTokens) > self.docMaxTokenNo_token_threshold):\n","                    self.docMaxTokenNo_token_threshold = len(thisTokens)\n","            else:\n","                self.my_deleted_bug[i] = len(thisTokens)\n","                del labels[i]\n","                del self.vector_tfidf[i]\n","            if (len(thisTokens) > self.docMaxTokenNo_org):\n","                self.docMaxTokenNo_org = len(thisTokens)\n","    \n","    \n","    def loadW2V(self, w2vpath):\n","        print(\"loadW2V\")\n","        with open(w2vpath, \"rb\") as f:\n","            header = f.readline()\n","            model_vocab_size, model_vector_size = map(int, header.split())\n","            binary_len = np.dtype(\"float32\").itemsize * model_vector_size\n","            \n","            for line_no in range(model_vocab_size):\n","                word = []\n","                while True:\n","                    ch = f.read(1)\n","                    if ch == b\" \":\n","                        break\n","                    if ch == b\"\":\n","                        raise EOFError(\"unexpected end of input; is count incorrect or file otherwise damaged?\")\n","                    if ch != b\"\\n\":\n","                        word.append(ch)\n","                word = b\"\".join(word).decode(\"utf-8\")\n","                if (word in self.w2vDic.keys()):\n","                    self.w2vDic[word] = np.frombuffer(f.read(binary_len), dtype=\"float32\")\n","                else:\n","                    f.seek(binary_len, 1)\n","    \n","    \n","    def vectorize_w2V (self, keywordBased=False):\n","        print(\"vectorize_w2V\")\n","        tempVec = []\n","        x = slice(0, self.docMaxLen)\n","        if keywordBased:\n","            print(\"Keyword Based\")\n","            for doc_tokens, doc_tfidf in zip(self.bugRepTokens, self.vector_tfidf):\n","                docKeywords = list(doc_tfidf.keys())[x]\n","                docAbs = [t for t in doc_tokens if t in docKeywords] # getDocAbsrtract_\n","                tempVec = [self.w2vDic[term] for term in docAbs]\n","                self.vector_em.append(tempVec)\n","                if (len(tempVec) > self.docMaxTokenNo):\n","                    self.docMaxTokenNo = len(tempVec)\n","        else:\n","            print(\"Not Keyword Based\")\n","            for doc_tokens in self.bugRepTokens:\n","                tempVec = [self.w2vDic[term] for term in doc_tokens]\n","                self.vector_em.append(tempVec)\n","                if (len(tempVec) > self.docMaxTokenNo):\n","                    self.docMaxTokenNo = len(tempVec)\n","    \n","    \n","    def padding(self):\n","        for doc in self.vector_em:\n","            if (len(doc) < self.docMaxTokenNo):\n","                doc.extend([self.paddingVector] * (self.docMaxTokenNo - len(doc)))\n","    \n","    \n","    def freeMem(self):\n","        self.w2vDic = {}\n","        self.bugRepTokens = []\n","        self.vector_tfidf = []\n","        self.vector_em = []"]},{"cell_type":"markdown","id":"064e2002","metadata":{"id":"064e2002"},"source":["## obj"]},{"cell_type":"code","execution_count":null,"id":"da0ae046","metadata":{"ExecuteTime":{"end_time":"2022-05-16T18:27:00.980453Z","start_time":"2022-05-16T18:25:42.714877Z"},"id":"da0ae046","executionInfo":{"status":"ok","timestamp":1658330554495,"user_tz":-270,"elapsed":40935,"user":{"displayName":"Ziba Ghane","userId":"08687858434532704051"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"dcfa04d1-63f4-44bd-9759-2f28058de0c5"},"outputs":[{"output_type":"stream","name":"stdout","text":["loadW2V\n","vectorize_w2V\n","Keyword Based\n"]}],"source":["ds = Preprocessing(preprocessing_params[\"docMaxLen\"], token_threshold)\n","ds.load_tfidf(mypaths[\"data\"][\"tfidf_word_weights\"])\n","ds.tokenize(texts)\n","\n","# --- vectorize: w2v (keywordbased or no)\n","ds.loadW2V(mypaths[\"data\"][\"w2v_word_vectors\"])\n","ds.vectorize_w2V(preprocessing_params[\"keyword_Based\"])"]},{"cell_type":"code","source":["our_input = []\n","for text_w2v in ds.vector_em:\n","    temp1 = np.array(text_w2v)\n","    temp2 = temp1.sum(axis=0)\n","    temp3 = list(temp2)\n","    our_input.append(temp3)"],"metadata":{"id":"9bCzmwzWDTGu"},"id":"9bCzmwzWDTGu","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Naive Bayes | linear_kernel"],"metadata":{"id":"hKDeUvwqFxOz"},"id":"hKDeUvwqFxOz"},{"cell_type":"code","source":["X_train, X_test, y_train, y_test = train_test_split(our_input, labels, random_state=0, train_size=0.75)"],"metadata":{"id":"zkoTZfMYl95g"},"id":"zkoTZfMYl95g","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# clf = MultinomialNB().fit(X_train, y_train)"],"metadata":{"id":"Ke6naje4jxJh"},"id":"Ke6naje4jxJh","execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.preprocessing import MinMaxScaler\n","from sklearn.pipeline import Pipeline\n","\n","p = Pipeline([('Normalizing',MinMaxScaler()),('MultinomialNB',MultinomialNB())])\n","p.fit(X_train, y_train) "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"niJWhPQXLJdr","executionInfo":{"status":"ok","timestamp":1658330555690,"user_tz":-270,"elapsed":544,"user":{"displayName":"Ziba Ghane","userId":"08687858434532704051"}},"outputId":"00f7ea4a-dafd-43e3-fb43-9c2706c0347a"},"id":"niJWhPQXLJdr","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Pipeline(steps=[('Normalizing', MinMaxScaler()),\n","                ('MultinomialNB', MultinomialNB())])"]},"metadata":{},"execution_count":21}]},{"cell_type":"code","source":["predicted = p.predict(X_test)\n","np.mean(predicted == y_test)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FyFF6S74mqFC","executionInfo":{"status":"ok","timestamp":1658330555690,"user_tz":-270,"elapsed":19,"user":{"displayName":"Ziba Ghane","userId":"08687858434532704051"}},"outputId":"e1933c1a-e525-44e4-a25e-2b0272cb4d29"},"id":"FyFF6S74mqFC","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.6718817905258583"]},"metadata":{},"execution_count":22}]},{"cell_type":"code","source":["print(len(predicted))\n","print(len(labels))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Xjg1hxYsF03I","executionInfo":{"status":"ok","timestamp":1658330555691,"user_tz":-270,"elapsed":17,"user":{"displayName":"Ziba Ghane","userId":"08687858434532704051"}},"outputId":"879d0548-69b9-4e96-e4cd-79b208726559"},"id":"Xjg1hxYsF03I","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2301\n","9201\n"]}]},{"cell_type":"code","source":["type(predicted)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r-56dea8njY9","executionInfo":{"status":"ok","timestamp":1658330555691,"user_tz":-270,"elapsed":14,"user":{"displayName":"Ziba Ghane","userId":"08687858434532704051"}},"outputId":"88955f8d-e5ac-4ed9-a5ef-18afa74487ca"},"id":"r-56dea8njY9","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["numpy.ndarray"]},"metadata":{},"execution_count":24}]},{"cell_type":"markdown","id":"d12f3786","metadata":{"id":"d12f3786"},"source":["# MyConfusionMatrix"]},{"cell_type":"code","execution_count":null,"id":"c94c4eda","metadata":{"ExecuteTime":{"end_time":"2022-05-13T12:48:28.427363Z","start_time":"2022-05-13T12:48:28.400379Z"},"id":"c94c4eda"},"outputs":[],"source":["class MyConfusionMatrix():\n","    def __init__(self, num_classes):\n","        # rows: actual, columns: prediction\n","        self.confusion_matrix = np.zeros((num_classes, num_classes), dtype=np.int32)\n","    \n","    \n","    def update(self, y, yhat_indices):\n","        for actual, pred in zip(y, yhat_indices):\n","                self.confusion_matrix[actual, pred] += 1\n","    \n","    \n","    def calc_accuracy(self):\n","        diagon = self.confusion_matrix.diagonal()\n","        # accuracy\n","        total_samples = self.confusion_matrix.sum()\n","        total_corrects = diagon.sum()\n","        accuracy = 100 * (total_corrects / total_samples)\n","        \n","        # accuracy per class\n","        # sum(1): 1 referes to sum for each row\n","        samples_per_class = self.confusion_matrix.sum(1)\n","        accuracy_per_class = 100 * (np.divide(diagon, samples_per_class))\n","        \n","        return accuracy, accuracy_per_class.tolist()\n","    \n","    \n","    def get_cf(self):\n","        return self.confusion_matrix.tolist()"]},{"cell_type":"markdown","source":["## obj"],"metadata":{"id":"efrXn6WUG1a8"},"id":"efrXn6WUG1a8"},{"cell_type":"code","execution_count":null,"id":"84bfb371","metadata":{"id":"84bfb371"},"outputs":[],"source":["confusion_matrix = MyConfusionMatrix(preprocessing_params[\"data\"][\"dataset\"][\"num_bug_classes\"])\n","confusion_matrix.update(y_test, predicted)"]},{"cell_type":"markdown","source":["# Save"],"metadata":{"id":"tt-99z70VccJ"},"id":"tt-99z70VccJ"},{"cell_type":"code","source":["def save_to_file_results(dataset_name, preprocessing_params, result_path):\n","    tempStructure = {\n","        \"dataset\": dataset_name,\n","        \"preprocessing_params\": preprocessing_params,\n","        \"model_results\": {\n","            \"confusion_matrix\": confusion_matrix.get_cf()\n","        }\n","    }\n","    \n","    with open(result_path, \"w\") as fout:\n","        json.dump(tempStructure, fout)"],"metadata":{"id":"ZBF7T2hNLnRD"},"id":"ZBF7T2hNLnRD","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# save_to_file_results(\n","#     mypaths[\"data\"][\"dataset\"], \n","#     preprocessing_params, \n","#     mypaths[\"output\"][\"performance\"]\n","# )"],"metadata":{"id":"JaI44H6-NGYd"},"id":"JaI44H6-NGYd","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# CalculateMetrics"],"metadata":{"id":"xemEd3mjLG-Y"},"id":"xemEd3mjLG-Y"},{"cell_type":"code","source":["class CalculateMetrics():\n","    def __init__(self, cm):\n","        self.cm = cm # it is a numpy object\n","        self.true_positives = np.diag(cm)\n","    \n","    \n","    # calculate precision for each class\n","    def calc_precision(self):\n","        columns_sum = np.sum(self.cm, axis=0)\n","        prec = list(map(truediv, self.true_positives, columns_sum))\n","        self.precision = prec\n","        return prec\n","    \n","    \n","    # calculate recall for each class\n","    # recall = accuracy per class\n","    # how accuratly each class is predicted\n","    def calc_recall(self):\n","        rows_sum = np.sum(self.cm, axis=1)\n","        rec = list(map(truediv, self.true_positives, rows_sum))\n","        self.recall = rec\n","        return rec\n","    \n","    \n","    # calculate f1_score for each class\n","    def calc_f1_score(self):\n","        tempPrec = np.array(self.precision)\n","        tempRec = np.array(self.recall)\n","        numerator = tempPrec * tempRec\n","        Denominator = tempPrec + tempRec\n","        f1s = 2 * (numerator / Denominator)\n","        self.f1_score = f1s\n","        return f1s\n","    \n","    def calc_accuracy(self):\n","        total_samples = np.sum(self.cm)\n","        sum_true_positives = sum(self.true_positives)\n","        acc = (sum_true_positives / total_samples)\n","        return acc"],"metadata":{"id":"-VmzVkjFKrKb"},"id":"-VmzVkjFKrKb","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## obj"],"metadata":{"id":"_3wap2Wrbyk9"},"id":"_3wap2Wrbyk9"},{"cell_type":"code","source":["cf_matrix = confusion_matrix.get_cf()\n","cf_matrix = np.array(cf_matrix)\n","            \n","calcmet = CalculateMetrics(cf_matrix)\n","precision = calcmet.calc_precision()\n","precision = [round(elem, 2) * 100 for elem in precision]\n","\n","recall = calcmet.calc_recall()\n","recall = [round(elem, 2) * 100 for elem in recall]\n","\n","f1_score = calcmet.calc_f1_score()\n","f1_score = [round(elem, 2) * 100 for elem in f1_score]\n","\n","acc = calcmet.calc_accuracy()\n","acc = round(acc * 100)"],"metadata":{"id":"XiKgRVbRLROX","executionInfo":{"status":"ok","timestamp":1658330556731,"user_tz":-270,"elapsed":1049,"user":{"displayName":"Ziba Ghane","userId":"08687858434532704051"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"f5a45e43-c1e6-4b44-e7fb-e3364e8203b8"},"id":"XiKgRVbRLROX","execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:10: RuntimeWarning: invalid value encountered in long_scalars\n","  # Remove the CWD from sys.path while we load stuff.\n"]}]},{"cell_type":"code","source":["print(\"-\" * 15)\n","print(\"accuracy           :\", acc)\n","print(\"precision          :\", precision)\n","print(\"recall             :\", recall)\n","print(\"f1_score           :\", f1_score)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d-TCXzmGHwHe","executionInfo":{"status":"ok","timestamp":1658330556734,"user_tz":-270,"elapsed":22,"user":{"displayName":"Ziba Ghane","userId":"08687858434532704051"}},"outputId":"37c8336f-7b25-4d7f-c441-a22a860c87c5"},"id":"d-TCXzmGHwHe","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["---------------\n","accuracy           : 67\n","precision          : [67.0, nan]\n","recall             : [100.0, 0.0]\n","f1_score           : [80.0, nan]\n"]}]},{"cell_type":"markdown","source":["# results"],"metadata":{"id":"Ang47NYUGZi8"},"id":"Ang47NYUGZi8"},{"cell_type":"code","source":["print(\"len(ds.bugRepTokens)   : \", len(ds.bugRepTokens))\n","print(\"ds.docMaxTokenNo_org   : \", ds.docMaxTokenNo_org)\n","print(\"len(ds.w2vDic)         : \", len(ds.w2vDic))\n","print(\"len(tfidf.vocabulary_) : \", len(tfidf.vocabulary_))\n","print(\"len(gfno)              : \", len(gfno))\n","print(\"vec.shape              : \", vec.shape)\n","print(\"len(texts)             : \", len(texts))\n","print(\"len(predicted)         : \", len(predicted))\n","print(\"len(labels)            : \", len(labels))"],"metadata":{"id":"ODuwtiUrVmkI","executionInfo":{"status":"error","timestamp":1658330556740,"user_tz":-270,"elapsed":26,"user":{"displayName":"Ziba Ghane","userId":"08687858434532704051"}},"colab":{"base_uri":"https://localhost:8080/","height":287},"outputId":"c6903098-f832-4c42-f632-9ca481fbc87b"},"id":"ODuwtiUrVmkI","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["len(ds.bugRepTokens)   :  9201\n","ds.docMaxTokenNo_org   :  8463\n","len(ds.w2vDic)         :  27969\n"]},{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-32-93fc9efee4d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ds.docMaxTokenNo_org   : \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocMaxTokenNo_org\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"len(ds.w2vDic)         : \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw2vDic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"len(tfidf.vocabulary_) : \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocabulary_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"len(gfno)              : \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgfno\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"vec.shape              : \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'tfidf' is not defined"]}]},{"cell_type":"code","source":["np.random.choice(\n","  ['pooh', 'rabbit', 'piglet', 'Christopher'], \n","  5,\n","  p=[0.5, 0.1, 0.1, 0.3]\n",")"],"metadata":{"id":"QJ_UBdTsHpjp"},"id":"QJ_UBdTsHpjp","execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"sTnKEliIK7Ny"},"id":"sTnKEliIK7Ny","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":false,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{"height":"calc(100% - 180px)","left":"10px","top":"150px","width":"190px"},"toc_section_display":true,"toc_window_display":true},"colab":{"provenance":[],"collapsed_sections":[],"toc_visible":true},"gpuClass":"standard"},"nbformat":4,"nbformat_minor":5}